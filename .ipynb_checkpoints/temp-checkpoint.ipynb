{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install face-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install face-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_faces_in_video(video_path, known_face_encodings, known_face_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images and encodings\n",
      "Video stopped by user.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from face_recognition import load_image_file, face_locations, face_encodings, compare_faces, face_distance\n",
    "\n",
    "# Step 1: Read and encode multiple input images\n",
    "def load_and_encode_images(image_paths):\n",
    "    encodings = []\n",
    "    names = []\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        # Load and convert the image to RGB\n",
    "        input_image = load_image_file(image_path)\n",
    "        rgb_input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect face(s) in the image using the CNN model\n",
    "        input_face_locations = face_locations(rgb_input_image, model=\"cnn\")  # Use CNN model for better accuracy\n",
    "        \n",
    "        # If no face detected, skip this image\n",
    "        if len(input_face_locations) == 0:\n",
    "            print(f\"No face detected in the image: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Compute the facial encoding for each detected face\n",
    "        input_face_encodings = face_encodings(rgb_input_image, input_face_locations)\n",
    "        \n",
    "        # Save the first encoding (assuming one face per image)\n",
    "        encodings.append(input_face_encodings[0])\n",
    "        names.append(image_path.split(\"/\")[-1].split(\".\")[0])\n",
    "    \n",
    "    return encodings, names\n",
    "\n",
    "# Step 2: Process video and search for multiple faces\n",
    "def search_faces_in_video(video_path, known_face_encodings, known_face_names, threshold=0.6, process_every_nth_frame=3):\n",
    "    # Open the video file\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "    frame_count = 0  # Track the frame count\n",
    "\n",
    "    while video_capture.isOpened():\n",
    "        # Read frame by frame\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            print(\"End of video reached or error reading video.\")\n",
    "            break  # End of video\n",
    "\n",
    "        frame_count += 1\n",
    "        # Process every nth frame to reduce CPU load\n",
    "        if frame_count % process_every_nth_frame != 0:\n",
    "            continue\n",
    "        \n",
    "        # Optionally resize frame for faster processing\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)  # Downscaling by 50%\n",
    "        rgb_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect faces in the current frame using the CNN model\n",
    "        frame_face_locations = face_locations(rgb_frame, model=\"cnn\")  # Use CNN model here\n",
    "        frame_face_encodings = face_encodings(rgb_frame, frame_face_locations)\n",
    "\n",
    "        if len(frame_face_encodings) == 0:\n",
    "            continue  # Skip if no faces were found\n",
    "\n",
    "        for (top, right, bottom, left), face_encoding in zip(frame_face_locations, frame_face_encodings):\n",
    "            # Compare the detected face with all known face encodings\n",
    "            face_distances = face_distance(known_face_encodings, face_encoding)\n",
    "\n",
    "            if len(face_distances) == 0:\n",
    "                continue  # Skip if no known faces are available\n",
    "\n",
    "            # Find the best match\n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            matches = compare_faces(known_face_encodings, face_encoding, tolerance=threshold)\n",
    "            \n",
    "            if matches[best_match_index]:\n",
    "                # Label the face with the corresponding known face name\n",
    "                name = known_face_names[best_match_index]\n",
    "\n",
    "                # Draw a box around the face (adjust for the downscaled frame)\n",
    "                top, right, bottom, left = [int(x * 2) for x in [top, right, bottom, left]]  # Scale up\n",
    "                cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, name, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            else:\n",
    "                # Label as unknown\n",
    "                top, right, bottom, left = [int(x * 2) for x in [top, right, bottom, left]]  # Scale up\n",
    "                cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "                cv2.putText(frame, \"Unknown\", (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "        # Display the frame with bounding boxes\n",
    "        cv2.imshow('Video', frame)\n",
    "        \n",
    "        # Handle keyboard interrupt and break if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Video stopped by user.\")\n",
    "            break\n",
    "\n",
    "    # Release the video capture object and close the display window\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Main function\n",
    "if __name__ == '__main__':\n",
    "    # Paths to multiple images and video\n",
    "    input_image_paths = [r'C:\\Users\\swath\\OneDrive\\Desktop\\face_detection\\woman.png']  # Add paths to all images\n",
    "    video_path = r\"C:\\Users\\swath\\OneDrive\\Desktop\\face_detection\\3205621-hd_1920_1080_25fps.mp4\"\n",
    "\n",
    "    # Step 1: Load and encode the multiple input images\n",
    "    known_face_encodings, known_face_names = load_and_encode_images(input_image_paths)\n",
    "    print(\"Loaded images and encodings\")\n",
    "    \n",
    "    # Step 2: Search for these faces in the video\n",
    "    search_faces_in_video(video_path, known_face_encodings, known_face_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
